# Phase 3B: Core Scoring Engine

**Status:** ~85% Complete (Functional but Missing Tests)
**Complexity:** HIGH
**Actual Implementation:** Manual (during Phase 3A)
**Duration:** Implemented alongside Phase 3A infrastructure
**Remaining Work:** See `PHASE_3B_COMPLETION_HANDOFF.md` for concise ADW handoff

---

‚ö†Ô∏è **FOR ADW WORKFLOW HANDOFF:** Use `docs/PHASE_3B_COMPLETION_HANDOFF.md` (230 lines)
üìÑ **THIS FILE:** Full specification and implementation details (756 lines)

---

## Implementation Summary

### ‚úÖ What's Been Implemented

**Core Scoring Functions** (app/server/core/workflow_analytics.py):
- ‚úÖ `calculate_nl_input_clarity_score()` (lines 14-76) - Fully functional
  - Word count scoring with sweet spot detection
  - Acceptance criteria keyword detection
  - Technical specificity analysis
  - Question vs statement ratio penalty

- ‚ö†Ô∏è `calculate_cost_efficiency_score()` (lines 78-123) - 90% complete
  - ‚úÖ Budget adherence scoring (actual vs estimated)
  - ‚úÖ Cache efficiency scoring (cache hit rate)
  - ‚úÖ Retry penalty calculation
  - ‚ùå **Missing:** Model appropriateness scoring (10-point component)

- ‚úÖ `calculate_performance_score()` (lines 125-188) - Fully functional
  - Duration comparison vs similar workflows
  - Bottleneck detection (>30% of total time)
  - Idle time penalty
  - Handles no-comparison-data case correctly

- ‚úÖ `calculate_quality_score()` (lines 190-234) - Fully functional
  - Error rate scoring
  - Retry count penalty
  - PR review cycles tracking
  - CI test pass rate bonus/penalty

- ‚úÖ `calculate_word_count()` (lines 236-241) - Simple helper

**Integration** (app/server/core/workflow_history.py):
- ‚úÖ `sync_workflow_history()` updated (lines 875-913)
  - Calculates all scores during workflow sync
  - Only processes completed/failed workflows
  - Handles missing data gracefully
  - Temporal data extraction (hour, day of week) inline at lines 886-887

**Bonus Features** (Beyond Phase 3B Scope):
- ‚≠ê `find_similar_workflows()` (lines 243-308) - Phase 3E feature implemented early
- ‚≠ê `detect_anomalies()` (lines 310-381) - Phase 3D feature implemented early
- ‚≠ê `generate_optimization_recommendations()` (lines 383-468) - Phase 3D feature implemented early

### ‚ùå What's Missing from Spec

**Functional Gaps:**
1. Model appropriateness component in `calculate_cost_efficiency_score()` (spec lines 106-111)
   - Should detect simple/medium/complex tasks
   - Should score Haiku vs Sonnet appropriateness
   - Currently missing 10 points of the score

2. Helper functions not extracted as reusable:
   - `extract_hour()` - Logic exists inline in workflow_history.py:886
   - `extract_day_of_week()` - Logic exists inline in workflow_history.py:887
   - `detect_complexity()` - Not implemented anywhere

**Critical Testing Gap:**
- ‚ùå **NO unit tests** created (spec required comprehensive suite)
- ‚ùå `app/server/tests/test_workflow_analytics.py` does not exist
- ‚ùå No TestClarityScoring, TestCostEfficiencyScoring, etc. classes
- ‚ùå No integration tests validating sync score population

**Impact:** Core functionality works and is integrated, but not production-ready without tests.

### Completion Status by Acceptance Criteria

‚úÖ = Complete | ‚ö†Ô∏è = Partial | ‚ùå = Missing

- ‚úÖ calculate_nl_input_clarity_score() implemented with scoring rubric
- ‚úÖ Clarity score handles edge cases (empty input, very long input)
- ‚ö†Ô∏è calculate_cost_efficiency_score() implemented (missing model appropriateness)
- ‚ö†Ô∏è Cost score handles missing estimate data (partial null handling)
- ‚úÖ calculate_performance_score() implemented with comparison logic
- ‚úÖ Performance score handles no similar workflows case
- ‚úÖ calculate_quality_score() implemented with error/retry/PR logic
- ‚úÖ Quality score handles missing PR/CI data gracefully
- ‚ùå extract_hour() helper created as standalone function
- ‚ùå extract_day_of_week() helper created as standalone function
- ‚ö†Ô∏è Temporal extraction handles different timestamp formats (limited)
- ‚úÖ sync_workflow_history() updated to calculate all scores
- ‚úÖ Sync process doesn't crash on missing data
- ‚úÖ All scores normalized to 0-100 range
- ‚ùå Unit tests created for each scoring function
- ‚ùå Unit tests cover edge cases (null values, extreme values)
- ‚ùå Integration test verifies sync populates score fields
- ‚úÖ No TypeScript/Python errors introduced
- ‚úÖ Backend server starts and runs sync successfully

**Score: 14/19 Acceptance Criteria Met (74%)**

## Overview

Implement the core analytics scoring functions that calculate metrics for workflow efficiency, performance, and quality. This is the "heavy lifting" phase that provides the intelligence behind the analytics dashboard.

## Why This Needs ADW

- **Complex algorithms** - Requires sophisticated scoring heuristics
- **Multiple scoring functions** - 4 distinct scoring systems to implement
- **Integration complexity** - Must understand existing workflow data structures
- **Testing requirements** - Needs comprehensive unit tests
- **Business logic validation** - Scoring formulas need careful review

## Dependencies

- **Phase 3A completed** - Infrastructure and stubs must exist
- **Phase 2 completed** - Performance metrics must be available
- Access to existing workflow data for testing

## Scope

### 1. NL Input Clarity Scoring

**Function:** `calculate_nl_input_clarity_score(nl_input: str) -> float`

**Scoring Criteria:**

```python
def calculate_nl_input_clarity_score(nl_input: str) -> float:
    """
    Calculate clarity score (0-100) based on input quality.

    Scoring breakdown:
    - Word count (30 points):
      - 0-20 words: 10 points
      - 21-50 words: 20 points
      - 51-300 words: 30 points (sweet spot)
      - 301-500 words: 25 points
      - 500+ words: 15 points (too verbose)

    - Acceptance criteria presence (25 points):
      - Contains "should", "must", "when": +8 points each
      - Contains bullet points: +5 points

    - Technical specificity (25 points):
      - Contains file paths: +10 points
      - Contains function/class names: +10 points
      - Contains technical terms: +5 points

    - Clarity indicators (20 points):
      - No questions (just statements): +10 points
      - Proper sentence structure: +5 points
      - Specific action verbs: +5 points

    Returns:
        Score from 0-100
    """
    # TODO: Implement scoring algorithm
    pass
```

**Test Cases:**
- Very short input (10 words): ~15-20 score
- Well-structured with criteria (150 words): ~80-90 score
- Verbose without structure (600 words): ~40-50 score
- Perfect input with all elements: ~95-100 score

### 2. Cost Efficiency Scoring

**Function:** `calculate_cost_efficiency_score(workflow: Dict) -> float`

**Scoring Criteria:**

```python
def calculate_cost_efficiency_score(workflow: Dict) -> float:
    """
    Calculate cost efficiency score (0-100).

    Scoring breakdown:
    - Budget adherence (40 points):
      - Under budget (<90% of estimate): 40 points
      - On budget (90-110%): 35 points
      - Slightly over (110-130%): 25 points
      - Significantly over (130-200%): 15 points
      - Way over (>200%): 0 points

    - Cache efficiency (30 points):
      - Cache hit rate >80%: 30 points
      - Cache hit rate 60-80%: 22 points
      - Cache hit rate 40-60%: 15 points
      - Cache hit rate <40%: 8 points

    - Retry rate (20 points):
      - 0 retries: 20 points
      - 1 retry: 15 points
      - 2 retries: 10 points
      - 3+ retries: 5 points

    - Model appropriateness (10 points):
      - Simple task with Haiku: 10 points
      - Complex task with Sonnet: 10 points
      - Overkill (Sonnet for simple): 5 points
      - Underpowered (Haiku for complex): 5 points

    Returns:
        Score from 0-100
    """
    # TODO: Implement scoring algorithm
    pass
```

**Test Cases:**
- Perfect workflow (under budget, high cache, no retries): ~95-100
- Average workflow (on budget, medium cache, 1 retry): ~70-75
- Inefficient workflow (over budget, low cache, retries): ~30-40

### 3. Performance Scoring

**Function:** `calculate_performance_score(workflow: Dict, similar_workflows: List[Dict]) -> float`

**Scoring Criteria:**

```python
def calculate_performance_score(workflow: Dict, similar_workflows: List[Dict]) -> float:
    """
    Calculate performance score (0-100) by comparing to similar workflows.

    Scoring breakdown:
    - Duration comparison (50 points):
      - Faster than average: 50 points
      - Within 10% of average: 45 points
      - 10-30% slower: 35 points
      - 30-50% slower: 20 points
      - >50% slower: 10 points

    - Bottleneck presence (30 points):
      - No bottleneck phases: 30 points
      - One bottleneck phase: 20 points
      - Multiple bottlenecks: 10 points

    - Idle time percentage (20 points):
      - <5% idle time: 20 points
      - 5-10% idle time: 15 points
      - 10-20% idle time: 10 points
      - >20% idle time: 5 points

    Note: If no similar workflows, score based on absolute duration:
    - <5 minutes: 90 points
    - 5-15 minutes: 70 points
    - 15-30 minutes: 50 points
    - >30 minutes: 30 points

    Returns:
        Score from 0-100
    """
    # TODO: Implement scoring algorithm
    pass
```

**Test Cases:**
- Faster than similar workflows: ~90-95
- Average performance: ~65-75
- Slower than similar: ~30-40
- First workflow (no comparison): Based on absolute duration

### 4. Quality Scoring

**Function:** `calculate_quality_score(workflow: Dict) -> float`

**Scoring Criteria:**

```python
def calculate_quality_score(workflow: Dict) -> float:
    """
    Calculate quality score (0-100).

    Scoring breakdown:
    - Error rate (40 points):
      - 0 errors: 40 points
      - 1-2 errors: 30 points
      - 3-5 errors: 20 points
      - 6-10 errors: 10 points
      - >10 errors: 5 points

    - Retry count (30 points):
      - 0 retries: 30 points
      - 1 retry: 22 points
      - 2 retries: 15 points
      - 3+ retries: 8 points

    - PR review cycles (20 points):
      - 1 cycle (approved first try): 20 points
      - 2 cycles: 15 points
      - 3 cycles: 10 points
      - 4+ cycles: 5 points
      - Not tracked: 10 points (neutral)

    - CI test pass rate (10 points):
      - 100% pass: 10 points
      - 90-99% pass: 8 points
      - 80-89% pass: 6 points
      - <80% pass: 3 points
      - Not tracked: 5 points (neutral)

    Returns:
        Score from 0-100
    """
    # TODO: Implement scoring algorithm
    pass
```

**Test Cases:**
- Perfect execution (no errors, no retries): ~90-100
- Minor issues (1-2 errors, 1 retry): ~70-80
- Quality problems (many errors, retries): ~30-40

### 5. Integration with Sync Process

**File:** `app/server/core/workflow_history.py`

Update `sync_workflow_history()` to calculate scores:

```python
def sync_workflow_history() -> int:
    """
    Sync workflow history from state files with analytics scoring.

    Enhanced to calculate:
    - NL input clarity scores
    - Cost efficiency scores
    - Performance scores (needs similar workflows)
    - Quality scores
    - Temporal data (hour, day of week)
    """
    # ... existing sync logic ...

    for workflow in workflows:
        # Calculate temporal data
        workflow['submission_hour'] = extract_hour(workflow['created_at'])
        workflow['submission_day_of_week'] = extract_day_of_week(workflow['created_at'])

        # Calculate word count
        if workflow.get('nl_input'):
            workflow['nl_input_word_count'] = len(workflow['nl_input'].split())
            workflow['nl_input_clarity_score'] = calculate_nl_input_clarity_score(
                workflow['nl_input']
            )

        # Calculate efficiency scores
        workflow['cost_efficiency_score'] = calculate_cost_efficiency_score(workflow)
        workflow['quality_score'] = calculate_quality_score(workflow)

        # Performance score needs similar workflows (defer to later in sync)
        # Similar workflows detection happens in Phase 3E

    # After all workflows synced, calculate performance scores
    for workflow in workflows:
        similar = find_similar_workflows(workflow, workflows)  # Phase 3E
        workflow['performance_score'] = calculate_performance_score(workflow, similar)

    # ... rest of sync logic ...
```

### 6. Helper Functions

**Temporal Data Extraction:**

```python
from datetime import datetime

def extract_hour(timestamp: str) -> int:
    """Extract hour (0-23) from ISO timestamp."""
    dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
    return dt.hour

def extract_day_of_week(timestamp: str) -> int:
    """Extract day of week (0=Monday, 6=Sunday) from ISO timestamp."""
    dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
    return dt.weekday()
```

**Complexity Detection:**

```python
def detect_complexity(workflow: Dict) -> str:
    """
    Detect workflow complexity for model appropriateness scoring.

    Returns: "simple", "medium", "complex"
    """
    word_count = workflow.get('nl_input_word_count', 0)
    duration = workflow.get('total_duration_seconds', 0)
    error_count = len(workflow.get('errors', []))

    if word_count < 50 and duration < 300 and error_count < 3:
        return "simple"
    elif word_count > 200 or duration > 1800 or error_count > 5:
        return "complex"
    else:
        return "medium"
```

## Acceptance Criteria

**Updated with Implementation Status:**

- [x] `calculate_nl_input_clarity_score()` implemented with scoring rubric ‚úÖ (workflow_analytics.py:14-76)
- [x] Clarity score handles edge cases (empty input, very long input) ‚úÖ
- [~] `calculate_cost_efficiency_score()` implemented with budget/cache/retry logic ‚ö†Ô∏è (missing model appropriateness)
- [~] Cost score handles missing estimate data gracefully ‚ö†Ô∏è (partial handling)
- [x] `calculate_performance_score()` implemented with comparison logic ‚úÖ (workflow_analytics.py:125-188)
- [x] Performance score handles no similar workflows case ‚úÖ
- [x] `calculate_quality_score()` implemented with error/retry/PR logic ‚úÖ (workflow_analytics.py:190-234)
- [x] Quality score handles missing PR/CI data gracefully ‚úÖ
- [ ] `extract_hour()` and `extract_day_of_week()` helpers created ‚ùå (logic exists inline, not as standalone functions)
- [~] Temporal extraction handles different timestamp formats ‚ö†Ô∏è (handles ISO format with Z suffix only)
- [x] `sync_workflow_history()` updated to calculate all scores ‚úÖ (workflow_history.py:875-913)
- [x] Sync process doesn't crash on missing data ‚úÖ
- [x] All scores normalized to 0-100 range ‚úÖ
- [ ] Unit tests created for each scoring function ‚ùå **CRITICAL GAP**
- [ ] Unit tests cover edge cases (null values, extreme values) ‚ùå **CRITICAL GAP**
- [ ] Integration test verifies sync populates score fields ‚ùå **CRITICAL GAP**
- [x] No TypeScript/Python errors introduced ‚úÖ
- [x] Backend server starts and runs sync successfully ‚úÖ

**Legend:**
- [x] = Fully implemented ‚úÖ
- [~] = Partially implemented ‚ö†Ô∏è
- [ ] = Not implemented ‚ùå

**Completion: 14/19 criteria met (74%)**

**Remaining Work for 100% Completion:**
1. Add model appropriateness logic to `calculate_cost_efficiency_score()` (~30 min)
2. Extract inline temporal logic to standalone helper functions (~30 min)
3. Create comprehensive unit test suite (~2-3 hours) **‚Üê CRITICAL**

## Testing Requirements

### Unit Tests

**File:** `app/server/tests/test_workflow_analytics.py`

```python
import pytest
from core.workflow_analytics import (
    calculate_nl_input_clarity_score,
    calculate_cost_efficiency_score,
    calculate_performance_score,
    calculate_quality_score,
    extract_hour,
    extract_day_of_week,
)

class TestClarityScoring:
    def test_short_input_low_score(self):
        result = calculate_nl_input_clarity_score("fix bug")
        assert 10 <= result <= 25

    def test_optimal_input_high_score(self):
        input_text = """
        Implement user authentication with the following requirements:
        - Must support email/password login
        - Should include password reset flow
        - Must store passwords securely using bcrypt
        Update the UserController class and create AuthService.
        """
        result = calculate_nl_input_clarity_score(input_text)
        assert 80 <= result <= 95

    def test_verbose_input_medium_score(self):
        long_input = " ".join(["word"] * 600)
        result = calculate_nl_input_clarity_score(long_input)
        assert 15 <= result <= 50

class TestCostEfficiencyScoring:
    def test_under_budget_high_score(self):
        workflow = {
            'estimated_cost': 1.0,
            'actual_cost': 0.8,
            'cache_read_tokens': 8000,
            'cache_write_tokens': 2000,
            'input_tokens': 10000,
            'retry_count': 0,
            'model': 'claude-haiku-3.5',
            'nl_input_word_count': 50
        }
        result = calculate_cost_efficiency_score(workflow)
        assert 90 <= result <= 100

    def test_over_budget_low_score(self):
        workflow = {
            'estimated_cost': 1.0,
            'actual_cost': 2.5,
            'cache_read_tokens': 1000,
            'cache_write_tokens': 9000,
            'input_tokens': 10000,
            'retry_count': 3,
            'model': 'claude-sonnet-3.5'
        }
        result = calculate_cost_efficiency_score(workflow)
        assert result <= 40

# ... more test cases ...
```

### Integration Test

```python
def test_sync_calculates_all_scores():
    """Test that sync process calculates all score fields."""
    sync_workflow_history()

    workflows = get_all_workflows()
    assert len(workflows) > 0

    for workflow in workflows:
        if workflow.get('nl_input'):
            assert workflow['nl_input_clarity_score'] is not None
            assert 0 <= workflow['nl_input_clarity_score'] <= 100

        if workflow.get('estimated_cost'):
            assert workflow['cost_efficiency_score'] is not None
            assert 0 <= workflow['cost_efficiency_score'] <= 100

        assert workflow['quality_score'] is not None
        assert 0 <= workflow['quality_score'] <= 100
```

## Files to Create

- `app/server/tests/test_workflow_analytics.py` (unit tests)

## Files to Modify

- `app/server/core/workflow_analytics.py` (implement 4 scoring functions + helpers)
- `app/server/core/workflow_history.py` (integrate scoring into sync)

## Time Estimate

- Clarity scoring: 30 minutes
- Cost efficiency scoring: 45 minutes
- Performance scoring: 45 minutes
- Quality scoring: 30 minutes
- Helper functions: 15 minutes
- Sync integration: 30 minutes
- Unit tests: 45 minutes
- Testing/debugging: 30 minutes
- **Total: 4 hours**

## ADW Workflow Recommendations

**Classification:** `lightweight` or `standard` (depending on complexity)
**Model:** Sonnet (complex algorithms require reasoning)
**Issue Title:** "Implement Phase 3B: Core Scoring Engine for Workflow Analytics"

**Issue Description:**
```markdown
Implement the core scoring algorithms for workflow analytics as defined in Phase 3B.

Requirements:
- Implement 4 scoring functions with documented rubrics
- Add temporal extraction helpers
- Integrate scoring into sync process
- Create comprehensive unit tests
- Ensure all scores normalized to 0-100

See: docs/PHASE_3B_SCORING_ENGINE.md
```

## Success Metrics

- All 4 scoring functions return values 0-100
- Unit tests pass with >90% coverage
- Sync process populates all score fields
- No crashes on missing/null data
- Scores make intuitive sense when reviewed manually

## Next Phase

After Phase 3B is complete:
- **Phase 3C** will add UI to display these scores
- ScoreCard component will visualize efficiency metrics
- Users will see cost/performance/quality scores in workflow history

## Notes

- **Scoring is subjective** - Rubrics are starting points, can be tuned
- **Test with real data** - Use actual workflows to validate scores
- **Handle missing data gracefully** - Many fields may be null
- **Document assumptions** - Comment why certain thresholds chosen
- **Performance matters** - Sync runs on all workflows, keep calculations fast

## Risk Mitigation

1. **Complex algorithms** ‚Üí Start simple, iterate based on feedback
2. **Unclear requirements** ‚Üí Document assumptions, get user approval
3. **Missing data** ‚Üí Default to neutral scores (50) when data unavailable
4. **Performance issues** ‚Üí Profile sync process, optimize if needed
5. **Testing burden** ‚Üí Focus on edge cases, use property-based testing

## Alternative Approaches Considered

### Approach A: Simple Averaging
- Just average a few metrics
- **Rejected:** Not nuanced enough, doesn't capture complexity

### Approach B: Machine Learning
- Train ML model on historical data
- **Rejected:** Overkill for Phase 3B, consider for future

### Approach C: User-Defined Weights
- Let users configure scoring weights
- **Deferred:** Good idea, add in Phase 4

## Questions for User (Before ADW Launch)

1. Should scores default to `null` or `50` when data is missing?
   - **Current Implementation:** Defaults to 0.0 for most scores when data missing
2. Are the threshold values (e.g., 2x average = anomaly) reasonable?
   - **Current Implementation:** Uses 2x threshold for cost/duration anomalies
3. Should we track which scoring version was used (for future changes)?
   - **Not implemented:** No versioning tracking currently

---

## Implementation Notes (Current State)

### What Works Right Now

The scoring engine is **functional and integrated**:

1. **All 4 core scoring functions work** and return 0-100 scores
2. **Scores are automatically calculated** during `sync_workflow_history()`
3. **Database stores all score fields** (Phase 3A infrastructure complete)
4. **Backend/frontend type sync** is perfect (no TypeScript errors)
5. **Handles missing data gracefully** without crashing

**You can start using the scores immediately** - they're being calculated and stored for all completed workflows.

### Differences from Original Spec

**Weight Distribution:**
- The implemented scoring uses slightly different weight distributions than the spec
- All components are present, just weighted differently
- Scores are still normalized 0-100 and semantically meaningful

**Bonus Features Implemented Early:**
- `find_similar_workflows()` - Originally planned for Phase 3E
- `detect_anomalies()` - Originally planned for Phase 3D
- `generate_optimization_recommendations()` - Originally planned for Phase 3D

This means **Phases 3D and 3E have partial backend implementation already complete**.

### To Complete Phase 3B to 100%

**Required for Production Readiness:**

1. **Unit Test Suite** (~2-3 hours) **‚Üê HIGHEST PRIORITY**
   ```bash
   # Create: app/server/tests/test_workflow_analytics.py
   # With classes:
   # - TestClarityScoring
   # - TestCostEfficiencyScoring
   # - TestPerformanceScoring
   # - TestQualityScoring
   # - TestHelperFunctions
   # - TestIntegration
   ```

2. **Helper Function Refactor** (~30 min)
   - Extract `extract_hour()` from workflow_history.py:886 to workflow_analytics.py
   - Extract `extract_day_of_week()` from workflow_history.py:887 to workflow_analytics.py
   - Implement `detect_complexity()` function for model appropriateness

3. **Model Appropriateness Scoring** (~30 min)
   - Add 10-point component to `calculate_cost_efficiency_score()`
   - Detect task complexity (simple/medium/complex)
   - Score Haiku vs Sonnet appropriateness

### Recommended Next Steps

**Option A: Complete Phase 3B First (Recommended for Production)**
- Create comprehensive test suite
- Refactor helper functions
- Add model appropriateness scoring
- **Time:** 3-4 hours
- **Benefit:** Full confidence in scoring algorithms, production-ready

**Option B: Move to Phase 3C (Faster Iteration)**
- Accept Phase 3B at ~85% complete
- Build UI to visualize scores (Phase 3C)
- See scores in action with real data
- Return to complete tests based on real-world feedback
- **Time:** Start Phase 3C immediately
- **Benefit:** Faster user feedback, iterative improvement

**Option C: Hybrid Approach**
- Quick smoke tests (~30 min) to catch obvious bugs
- Move to Phase 3C for UI
- Full test suite in parallel or after Phase 3C
- **Time:** Balanced approach
- **Benefit:** Some test coverage + forward progress

### Files Modified During Implementation

**Created:**
- `app/server/db/migrations/003_add_analytics_metrics.sql` (36 lines)
- `app/server/core/workflow_analytics.py` (468 lines)

**Modified:**
- `app/server/core/data_models.py` - Added Phase 3A fields to WorkflowHistoryItem
- `app/client/src/types/api.types.ts` - Added Phase 3A fields to WorkflowHistoryItem
- `app/server/core/workflow_history.py` - Integrated scoring into sync (lines 875-913)

**Not Created (per spec):**
- `app/server/tests/test_workflow_analytics.py` - **Missing unit tests**

### Handoff to WebBuilder

If handing off to ADW workflow for completion:

**Issue Title:** "Complete Phase 3B: Add Tests and Missing Scoring Components"

**Issue Description:**
```markdown
Complete the remaining 26% of Phase 3B implementation:

**Required:**
1. Create comprehensive unit test suite in `app/server/tests/test_workflow_analytics.py`
   - TestClarityScoring (3+ test cases)
   - TestCostEfficiencyScoring (3+ test cases)
   - TestPerformanceScoring (3+ test cases)
   - TestQualityScoring (3+ test cases)
   - Test edge cases: null values, empty inputs, extreme values
   - Integration test: verify sync populates score fields

2. Extract helper functions to workflow_analytics.py:
   - extract_hour(timestamp: str) -> int
   - extract_day_of_week(timestamp: str) -> int
   - detect_complexity(workflow: Dict) -> str

3. Add model appropriateness to calculate_cost_efficiency_score():
   - Detect simple/medium/complex tasks
   - Score Haiku vs Sonnet appropriateness (+10 points max)

**Context:**
- Core scoring functions already implemented and working
- Database schema and types already updated
- Integration with sync process complete
- Just needs tests + minor enhancements for 100% completion

**See:** docs/PHASE_3B_SCORING_ENGINE.md (lines 52-71 for gap analysis)
```

**Classification:** lightweight (mostly tests, small refactor)
**Model:** Haiku (straightforward test writing)
**Estimated Cost:** $0.30-0.50
